{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70ccef4f",
   "metadata": {},
   "source": [
    "# TP3 run notebook (clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce8fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "DECADE_START = 1950\n",
    "DECADE_END = 1959\n",
    "N_CLUSTERS = 6\n",
    "TOP_N_TERMS = 15,\n",
    "OUT_DIR = 'tps/tp3'\n",
    "import os\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "print('params set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d99e950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "nltk.download('punkt', quiet=True)\n",
    "data_txt = Path('data/txt')\n",
    "files = [p for p in sorted(data_txt.glob('*.txt')) if re.search(r'(18|19)\\d{2}', p.name) and DECADE_START <= int(re.search(r'(18|19)\\d{2}', p.name).group(0)) <= DECADE_END]\n",
    "print('found', len(files), 'files')\n",
    "docs = []\n",
    "names = []\n",
    "for p in files:\n",
    "    t = p.read_text(encoding='utf-8')\n",
    "    t = re.sub(r'\\s+', ' ', t)\n",
    "    docs.append(t)\n",
    "    names.append(p.name)\n",
    "print('loaded docs', len(docs))\n",
    "if docs:\n",
    "    vectorizer = TfidfVectorizer(max_df=0.6, min_df=2, ngram_range=(1,2), stop_words='english')\n",
    "    X = vectorizer.fit_transform(docs)\n",
    "    kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=42, n_init=10)\n",
    "    kmeans.fit(X)\n",
    "    labels = kmeans.labels_\n",
    "    df = pd.DataFrame({'filename': names, 'cluster': labels})\n",
    "    df.to_csv(os.path.join(OUT_DIR, f'clusters_{DECADE_START}_{DECADE_END}.csv'), index=False)\n",
    "    print('saved clusters csv')\n",
    "    order = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    top_terms = {}\n",
    "    for i in range(N_CLUSTERS):\n",
    "        top = [terms[idx] for idx in order[i, :TOP_N_TERMS]]\n",
    "        top_terms[i] = top\n",
    "    pd.DataFrame.from_dict(top_terms, orient='index').to_csv(os.path.join(OUT_DIR, f'cluster_top_terms_{DECADE_START}_{DECADE_END}.csv'), header=False)\n",
    "    print('saved top terms csv')\n",
    "    # wordclouds\n",
    "    for i in range(N_CLUSTERS):\n",
    "        members = df[df.cluster==i].filename.tolist()\n",
    "        text = ''\n",
    "        for m in members:\n",
    "            idx = names.index(m)\n",
    "            text += docs[idx] + '\\n'\n",
    "        if not text.strip():\n",
    "            continue\n",
    "        wc = WordCloud(width=600, height=300, background_color='white').generate(text)\n",
    "        out = os.path.join(OUT_DIR, f'cluster_{i}_wordcloud_{DECADE_START}_{DECADE_END}.png')\n",
    "        wc.to_file(out)\n",
    "        print('saved', out)\n",
    "# sentences and word2vec\n",
    "sents = []\n",
    "for p in files:\n",
    "    t = p.read_text(encoding='utf-8')\n",
    "    for s in sent_tokenize(t, language='french'):\n",
    "        toks = [w.lower() for w in word_tokenize(s) if re.search('[a-zA-Z0-9]', w)]\n",
    "        if len(toks)>2:\n",
    "            sents.append(toks)\n",
    "print('built sents', len(sents))\n",
    "if sents:\n",
    "    model = Word2Vec(sentences=sents, vector_size=64, window=5, min_count=5, workers=4, epochs=5)\n",
    "    model.save(os.path.join(OUT_DIR, f'word2vec_{DECADE_START}_{DECADE_END}.model'))\n",
    "    print('saved w2v model')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
