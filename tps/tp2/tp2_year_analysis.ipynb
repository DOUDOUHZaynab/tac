{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eb9afb4",
   "metadata": {},
   "source": [
    "# Analyse par année — pipeline récapitulatif\n",
    "\n",
    "Ce notebook exécute, pour l'année choisie (ici 1955), les étapes demandées :\n",
    "1) Extraire les fichiers du corpus pour l'année,\n",
    "2) Extraire les mots-clés (fréquence) et itérativement enrichir la liste de stopwords,\n",
    "3) Générer un nuage de mots (image sauvegardée),\n",
    "4) Exécuter la reconnaissance d'entités nommées (personnes, organisations, lieux),\n",
    "5) Sélectionner 10 phrases et analyser leur polarité/subjectivité (tableau sauvegardé),\n",
    "6) Résumer et exporter les résultats dans `tps/tp2/` (image + CSV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eedd6a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year set to 1955, data dir ../../data/txt, outputs in .\n"
     ]
    }
   ],
   "source": [
    "# Paramètres\n",
    "YEAR = 1955\n",
    "DATA_TXT_DIR = '../../data/txt'  # chemin relatif depuis ce notebook\n",
    "OUT_DIR = '.'  # répertoire du notebook (tps/tp2)\n",
    "WORDCLOUD_PATH = 'wordcloud_1955.png'\n",
    "SENTIMENT_CSV = 'sentiment_1955.csv'\n",
    "N_TOP_WORDS = 100\n",
    "SENTENCE_SAMPLE_SIZE = 10\n",
    "print(f'Year set to {YEAR}, data dir {DATA_TXT_DIR}, outputs in {OUT_DIR}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b267265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 files for year 1955\n",
      "Top words: [('tél', 1926), ('on', 1773), ('fr', 1660), ('qu', 1621), ('rue', 1593), ('000', 1558), ('rossel', 1505), ('dem', 1426), ('ag', 1363), ('10', 1214), ('ch', 1169), ('15', 1086), ('bruxelles', 1042), ('11', 1014), ('20', 988), ('30', 930), ('pr', 902), ('17', 895), ('12', 856), ('cette', 852)]\n",
      "Saved wordcloud to wordcloud_1955.png\n",
      "Saved sentiment CSV to sentiment_1955.csv\n"
     ]
    }
   ],
   "source": [
    "# Nettoyage, tokenisation, fréquence et génération du nuage de mots\n",
    "import re, os, string, random\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "# stopwords de base (français)\n",
    "BASE_STOPWORDS = set([\n",
    "    'le','la','les','un','une','de','des','du','et','en','à','a','au','aux',\n",
    "    'dans','pour','par','avec','ce','ces','se','qui','que','est','sont',\n",
    "    'sur','pas','ne','il','elle','nous','vous','ils','elles','sa','son','ses',\n",
    "    'mais','ou','donc','or','ni','car','comme','plus','moins','aussi','été','être'\n",
    "])\n",
    "# Récupérer les fichiers du corpus pour l'année\n",
    "p = Path(DATA_TXT_DIR)\n",
    "files = [f for f in p.iterdir() if f.is_file() and str(YEAR) in f.name]\n",
    "print(f'Found {len(files)} files for year {YEAR}')\n",
    "corpus = ''\n",
    "for f in files:\n",
    "    try:\n",
    "        corpus += f.read_text(encoding='utf-8') + '\\n'\n",
    "    except Exception:\n",
    "        corpus += f.read_text(encoding='latin-1') + '\\n'\n",
    "# Tokenisation simple par mots\n",
    "words = re.findall(r'\\w+', corpus.lower(), flags=re.UNICODE)\n",
    "words = [w.strip(string.punctuation) for w in words if len(w) > 1]\n",
    "words = [w for w in words if not re.fullmatch(r'\\\\d+', w)]\n",
    "counter = Counter(words)\n",
    "# filtrer stopwords initialement\n",
    "freq = [(w, c) for w, c in counter.most_common() if w not in BASE_STOPWORDS][:N_TOP_WORDS]\n",
    "print('Top words:', freq[:20])\n",
    "# Générer le nuage de mots et sauvegarder\n",
    "wc = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(freq))\n",
    "out_path = Path(OUT_DIR) / WORDCLOUD_PATH\n",
    "wc.to_file(str(out_path))\n",
    "print('Saved wordcloud to', out_path)\n",
    "# Extraction de phrases et analyse de sentiments (TextBlob)\n",
    "# Ensure NLTK sentence tokenizer resources for French are available\n",
    "nltk.download('punkt', quiet=True)\n",
    "# Some installations require the 'punkt_tab' french model; try to fetch it too\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from textblob import TextBlob\n",
    "sents = sent_tokenize(corpus, language='french') if corpus else []\n",
    "if len(sents) == 0:\n",
    "    print('No sentences found for sentiment analysis')\n",
    "sample = sents if len(sents) <= SENTENCE_SAMPLE_SIZE else random.sample(sents, SENTENCE_SAMPLE_SIZE)\n",
    "rows = []\n",
    "for s in sample:\n",
    "    tb = TextBlob(s)\n",
    "    rows.append({'sentence': s, 'polarity': tb.sentiment.polarity, 'subjectivity': tb.sentiment.subjectivity})\n",
    "df = pd.DataFrame(rows)\n",
    "csv_path = Path(OUT_DIR) / SENTIMENT_CSV\n",
    "df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "print('Saved sentiment CSV to', csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7537a25b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tac_venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
