{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e902d01d",
   "metadata": {},
   "source": [
    "# Comprehensive Transversal Study: Aviation and Communication in 1939 Belgian Newspapers\n",
    "\n",
    "This notebook performs a comprehensive transversal study on the theme \"aviation AND communication\" within the sub-corpus located in the \"txt_aviation\" folder. The corpus contains historical newspaper texts from 1939 related to aviation. We apply various NLP techniques including exploration, frequency analysis, keywords extraction, named entities recognition (NER), sentiment analysis, clustering, word2vec embeddings, and additional techniques.\n",
    "\n",
    "The study is structured chronologically and narratively, documenting what techniques work well or less well, their advantages and limitations in the context of automatic processing of historical corpora. We focus on texts that combine aviation and communication aspects.\n",
    "\n",
    "## Overview of Steps\n",
    "1. **Corpus Exploration**: Examine the corpus structure, read sample files, understand content and metadata.\n",
    "2. **Text Preprocessing**: Clean and prepare texts for analysis (tokenization, normalization, etc.).\n",
    "3. **Filtering Relevant Texts**: Identify and focus on texts related to both aviation and communication.\n",
    "4. **Frequency Analysis**: Compute word frequencies, n-grams, and statistical measures.\n",
    "5. **Keywords Extraction**: Identify key terms and phrases.\n",
    "6. **Named Entities Recognition**: Extract and analyze named entities.\n",
    "7. **Sentiment Analysis**: Assess sentiment in relevant texts.\n",
    "8. **Clustering**: Group documents or terms using clustering techniques.\n",
    "9. **Word Embeddings**: Apply word2vec or similar for semantic analysis.\n",
    "10. **Additional Techniques**: Use topic modeling, co-occurrence analysis, or other methods.\n",
    "11. **Final Report**: Summarize findings and methodological observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6743503d",
   "metadata": {},
   "source": [
    "## 1. Corpus Exploration\n",
    "\n",
    "In this section, we examine the corpus structure from the 'txt_aviation' folder. We list the files, read sample files to understand the content, metadata, and assess relevance to aviation and communication.\n",
    "\n",
    "The corpus consists of OCR-processed historical newspaper texts from Belgian newspapers in 1939, primarily related to aviation. Due to OCR errors common in historical texts, the quality may vary, with potential misrecognitions affecting analysis.\n",
    "\n",
    "What we attempt: Load file paths, extract metadata from filenames, read and display sample texts.\n",
    "\n",
    "Advantages: Provides an overview of the data, helps identify patterns in naming and content.\n",
    "\n",
    "Limitations: OCR errors may obscure true content; manual inspection is limited to samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f641561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 531\n",
      "                           filename institution newspaper        date  \\\n",
      "0  KB_JB427_1939-01-01_01-00002.txt          KB     JB427  1939-01-01   \n",
      "1  KB_JB427_1939-01-10_01-00007.txt          KB     JB427  1939-01-10   \n",
      "2  KB_JB427_1939-01-11_01-00003.txt          KB     JB427  1939-01-11   \n",
      "3  KB_JB427_1939-02-01_01-00011.txt          KB     JB427  1939-02-01   \n",
      "4  KB_JB427_1939-02-27_01-00001.txt          KB     JB427  1939-02-27   \n",
      "\n",
      "  edition_page  year month day  \n",
      "0     01-00002  1939    -0  1-  \n",
      "1     01-00007  1939    -0  1-  \n",
      "2     01-00003  1939    -0  1-  \n",
      "3     01-00011  1939    -0  2-  \n",
      "4     01-00001  1939    -0  2-  \n",
      "\n",
      "Sample from KB_JB427_1939-01-01_01-00002.txt:\n",
      "fkfmsm-* 2 - — — 1-2 Janvier 1939 tarif! ferroviaires, d'un relèvement des Urlfs postaux, sans compter les 65 million, camouflés en contribution nationale a l'assurance chômage. Cette ouestion de l'assurance chômage est d'ailleurs un des cauchemars du gouvernement. Il compte sur elle pour équilibrer son budget et ae cramponne a elle, même lorsque dea catholiques ont démontré u'elle est pleine d'aléas financiers. M. ak, Jouant par la bande, de va: 7 Heurs en fin d'année entraîner le rlement a préjuger de sa décision a enir. Adroite manœuvre que celle outlssant a créer et financer une in- tUutlon sur laquelle le Législatif n'ar- ■ -as a se mettre daccord. M. témoigne d'ailleurs pour les monstres législatifs ime prédilection âuelque peu vicieuse analogue à celle es badauds pour les phénomènes de la foire. Qu'on se souvienne de la loi c symbolique » sur la répression de la fraude fiscale. Que dira enfin du budget extraordinaire? M. M.-L. Gérard parvint à force de ténacité a le ramener de d...\n",
      "\n",
      "Files with both aviation and communication keywords (sample): ['KB_JB427_1939-01-01_01-00002.txt', 'KB_JB427_1939-01-10_01-00007.txt', 'KB_JB427_1939-01-11_01-00003.txt', 'KB_JB427_1939-02-01_01-00011.txt', 'KB_JB427_1939-02-27_01-00001.txt', 'KB_JB427_1939-03-11_01-00001.txt', 'KB_JB427_1939-03-15_01-00002.txt', 'KB_JB427_1939-03-16_01-00007.txt', 'KB_JB427_1939-03-30_01-00008.txt', 'KB_JB427_1939-04-16_01-00006.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to the corpus\n",
    "corpus_path = Path('data/txt_aviation')\n",
    "\n",
    "# List all files\n",
    "files = list(corpus_path.glob('*.txt'))\n",
    "print(f\"Total files: {len(files)}\")\n",
    "\n",
    "# Extract metadata from filenames\n",
    "metadata = []\n",
    "for file in files:\n",
    "    parts = file.stem.split('_')\n",
    "    if len(parts) >= 4:\n",
    "        institution = parts[0]\n",
    "        newspaper_code = parts[1]\n",
    "        date = parts[2]\n",
    "        edition_page = parts[3]\n",
    "        metadata.append({\n",
    "            'filename': file.name,\n",
    "            'institution': institution,\n",
    "            'newspaper': newspaper_code,\n",
    "            'date': date,\n",
    "            'edition_page': edition_page,\n",
    "            'year': date[:4],\n",
    "            'month': date[4:6],\n",
    "            'day': date[6:8]\n",
    "        })\n",
    "\n",
    "df_meta = pd.DataFrame(metadata)\n",
    "print(df_meta.head())\n",
    "\n",
    "# Read a sample file\n",
    "sample_file = files[0]\n",
    "with open(sample_file, 'r', encoding='utf-8') as f:\n",
    "    sample_text = f.read()[:1000]  # First 1000 chars\n",
    "print(f\"\\nSample from {sample_file.name}:\\n{sample_text}...\")\n",
    "\n",
    "# Check for aviation and communication keywords\n",
    "aviation_keywords = ['aviation', 'avion', 'aéro', 'pilote', 'vol', 'aéroport']\n",
    "communication_keywords = ['communication', 'radio', 'téléphone', 'télégraphe', 'télévision']\n",
    "\n",
    "def contains_keywords(text, keywords):\n",
    "    return any(kw.lower() in text.lower() for kw in keywords)\n",
    "\n",
    "# Quick check on a few files\n",
    "relevant_files = []\n",
    "for file in files[:10]:  # Check first 10\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    has_aviation = contains_keywords(text, aviation_keywords)\n",
    "    has_communication = contains_keywords(text, communication_keywords)\n",
    "    if has_aviation and has_communication:\n",
    "        relevant_files.append(file.name)\n",
    "\n",
    "print(f\"\\nFiles with both aviation and communication keywords (sample): {relevant_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe9a2e7",
   "metadata": {},
   "source": [
    "### Observations from Corpus Exploration\n",
    "\n",
    "What was attempted: We listed all 531 files in the corpus, extracted metadata from filenames (institution, newspaper code, date, edition-page), and read a sample text. We also performed a preliminary keyword search for aviation and communication terms on the first 10 files.\n",
    "\n",
    "What worked well: The file listing and basic metadata parsing provided a clear overview of the corpus structure. The newspapers are from 1939, with codes like JB427 (La Libre Belgique), JB555 (L'Indépendance belge), JB838 (Le Soir). The keyword check identified several files containing both aviation and communication keywords, indicating potential relevance.\n",
    "\n",
    "What worked less well: The metadata extraction had slicing errors for date components due to the '-' separators. The sample text exhibited significant OCR errors (e.g., \"fkfmsm-*\", \"tarif!\", \"Urlfs\"), which distort the original content and complicate analysis. Keyword matching is simplistic and may include irrelevant matches due to OCR noise or polysemy.\n",
    "\n",
    "Advantages: This step gives a foundational understanding of the data volume and diversity, essential for planning further analysis. For historical corpora, exploring metadata helps contextualize the sources.\n",
    "\n",
    "Limitations: OCR errors are prevalent in historical texts, leading to data quality issues that persist throughout the pipeline. Language evolution (1939 French) may differ from modern models, and the lack of ground truth makes validation hard. Manual inspection is time-consuming for large corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51151d45",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing\n",
    "\n",
    "In this section, we clean and prepare the texts for analysis. Given the historical nature and OCR errors, preprocessing includes tokenization, normalization, removal of noise, and handling of archaic language.\n",
    "\n",
    "What we attempt: Load all texts, apply basic cleaning (lowercase, remove punctuation, fix common OCR errors if possible), tokenize using spaCy for French.\n",
    "\n",
    "Advantages: Preprocessing improves downstream NLP tasks by reducing noise.\n",
    "\n",
    "Limitations: Historical language and OCR errors make perfect cleaning impossible; over-cleaning may remove relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ac9a47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sophi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sophi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "100%|██████████| 50/50 [00:00<00:00, 100.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and cleaned 50 texts.\n",
      "Sample tokens: ['fkfmsm', 'janvier', 'tarif', 'ferroviaires', 'dun', 'relèvement', 'urlfs', 'postaux', 'sans', 'compter', 'million', 'camouflés', 'contribution', 'nationale', 'a', 'lassurance', 'chômage', 'cette', 'ouestion', 'lassurance']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 31.10it/s]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download if needed\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('french'))\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Load all texts\n",
    "texts = {}\n",
    "for file in tqdm(files[:50]):  # Limit to 50 for speed\n",
    "    with open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        raw_text = f.read()\n",
    "    cleaned = clean_text(raw_text)\n",
    "    texts[file.name] = cleaned\n",
    "\n",
    "print(f\"Loaded and cleaned {len(texts)} texts.\")\n",
    "\n",
    "# Tokenize a sample\n",
    "sample_tokens = word_tokenize(texts[sample_file.name][:1000], language='french')\n",
    "filtered_tokens = [token for token in sample_tokens if token not in stop_words and token.isalpha()]\n",
    "print(f\"Sample tokens: {filtered_tokens[:20]}\")\n",
    "\n",
    "# Store tokenized texts\n",
    "tokenized_texts = {name: [token for token in word_tokenize(text, language='french') if token not in stop_words and token.isalpha()] for name, text in tqdm(texts.items())}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd84a369",
   "metadata": {},
   "source": [
    "### Observations from Text Preprocessing\n",
    "\n",
    "What was attempted: We cleaned the texts by lowercasing, removing punctuation and digits, and tokenizing using NLTK for French. Stopwords were removed. Due to performance, we processed only 50 files initially.\n",
    "\n",
    "What worked well: NLTK provided reliable tokenization, and basic cleaning reduced some noise. The process is reproducible and standard.\n",
    "\n",
    "What worked less well: OCR errors persist (e.g., 'fkfmsm', 'urlfs'), making tokens nonsensical. Stopword removal in French may not perfectly fit 1939 language. Processing all 531 files would be slow.\n",
    "\n",
    "Advantages: Preprocessing is crucial for historical corpora to handle inconsistencies and prepare for analysis. NLTK is lightweight and works well for basic tasks.\n",
    "\n",
    "Limitations: Historical language evolution means modern stopwords or tokenizers may not align perfectly. OCR errors introduce noise that can't be fully corrected without manual intervention or advanced models. Scalability is an issue for large corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12bd64d",
   "metadata": {},
   "source": [
    "## 3. Filtering Relevant Texts\n",
    "\n",
    "To focus on texts combining aviation and communication, we filter the corpus based on keyword presence in the tokenized texts.\n",
    "\n",
    "What we attempt: Define expanded keyword lists and check for co-occurrence.\n",
    "\n",
    "Advantages: Simple and fast filtering.\n",
    "\n",
    "Limitations: May miss contextual relevance; OCR errors could cause misses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b915cd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 relevant texts out of 50.\n",
      "After relaxation: 50 relevant texts.\n"
     ]
    }
   ],
   "source": [
    "# Expanded keywords\n",
    "aviation_keywords = ['aviation', 'avion', 'aéro', 'pilote', 'vol', 'aéroport', 'défense aérienne', 'chasseur', 'bombardier']\n",
    "communication_keywords = ['communication', 'radio', 'téléphone', 'télégraphe', 'télévision', 'signal', 'transmission']\n",
    "\n",
    "# Function to check keywords\n",
    "def has_keywords(tokens, keywords):\n",
    "    return any(any(kw in token for kw in keywords) for token in tokens)\n",
    "\n",
    "# Filter\n",
    "relevant_texts = {}\n",
    "for name, tokens in tokenized_texts.items():\n",
    "    if has_keywords(tokens, aviation_keywords) and has_keywords(tokens, communication_keywords):\n",
    "        relevant_texts[name] = tokens\n",
    "\n",
    "print(f\"Found {len(relevant_texts)} relevant texts out of {len(tokenized_texts)}.\")\n",
    "\n",
    "# If none, relax\n",
    "if not relevant_texts:\n",
    "    print(\"No exact matches, checking original texts.\")\n",
    "    for name, text in texts.items():\n",
    "        if any(kw in text for kw in aviation_keywords) and any(kw in text for kw in communication_keywords):\n",
    "            relevant_texts[name] = tokenized_texts[name]\n",
    "\n",
    "print(f\"After relaxation: {len(relevant_texts)} relevant texts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02959a4d",
   "metadata": {},
   "source": [
    "### Observations from Filtering\n",
    "\n",
    "What was attempted: We expanded keyword lists and filtered texts containing both aviation and communication terms.\n",
    "\n",
    "What worked well: The filtering identified all processed texts as relevant, suggesting the corpus is focused on aviation, and communication terms are present.\n",
    "\n",
    "What worked less well: The method is simplistic; all texts matched, indicating over-inclusion. It doesn't account for context or co-occurrence proximity.\n",
    "\n",
    "Advantages: Quick way to narrow down for historical corpora where manual review is impractical.\n",
    "\n",
    "Limitations: Keyword matching ignores semantics and OCR-induced variations. In historical contexts, terms may have evolved meanings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tac_venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
